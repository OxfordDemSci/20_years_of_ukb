{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62f4f5d2",
   "metadata": {},
   "source": [
    "At a minimum it would be good to analyse the changing shape of the network over time, the changing shape of the Giant Component, the number of isolates, etc (subfigures to the right as linegraphs of counts of these two types of component)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd0d89b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import ast, functools\n",
    "from typing import Optional, List\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "df_all = pd.read_excel('../data/df_dimensions.xlsx', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a257786d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_all[['id','researchers','research_orgs','year']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4144f99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10846/10846 [00:09<00:00, 1188.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows parsed: 10846\n",
      "Rows still suspicious after repair: 0 (examples: [])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f13a51f229240a7b869eabc8a8300cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10846 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to safely parse 'researchers' field with multiple repair attempts\n",
    "import ast, json, re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def safe_parse_researchers(s):\n",
    "    \"\"\"\n",
    "    Return a list parsed from string s with multiple fallback/repair attempts.\n",
    "    \"\"\"\n",
    "    if pd.isna(s):\n",
    "        return []\n",
    "    # If it's already a list/tuple, return list\n",
    "    if isinstance(s, (list, tuple)):\n",
    "        return list(s)\n",
    "\n",
    "    orig = str(s)\n",
    "    # 1) try ast.literal_eval directly\n",
    "    try:\n",
    "        val = ast.literal_eval(orig)\n",
    "        return list(val) if isinstance(val, (list, tuple)) else val\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Make a cleaned copy: normalize smart quotes, remove non-printable control chars\n",
    "    s2 = orig.replace(\"’\", \"'\").replace(\"‘\", \"'\").replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "    s2 = re.sub(r\"[\\x00-\\x1f\\x7f-\\x9f]\", \"\", s2)  # strip control chars\n",
    "\n",
    "    # 2) Remove trailing commas before closing bracket: [a, b,] -> [a, b]\n",
    "    s2 = re.sub(r\",\\s*(\\])\", r\"\\1\", s2)\n",
    "\n",
    "    # 3) If it starts with [ but missing a closing ], append it\n",
    "    if s2.strip().startswith(\"[\") and not s2.strip().endswith(\"]\"):\n",
    "        s2 = s2 + \"]\"\n",
    "\n",
    "    # 4) Try json.loads after converting single quotes to double quotes (common fix)\n",
    "    try:\n",
    "        candidate = s2.replace(\"'\", '\"')\n",
    "        # also collapse duplicated commas like \", ,\"\n",
    "        candidate = re.sub(r\",\\s*,+\", \",\", candidate)\n",
    "        val = json.loads(candidate)\n",
    "        return list(val) if isinstance(val, (list, tuple)) else val\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 5) Try ast.literal_eval again on repaired s2\n",
    "    try:\n",
    "        val = ast.literal_eval(s2)\n",
    "        return list(val) if isinstance(val, (list, tuple)) else val\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 6) If quotes are unbalanced, try to balance single quotes by appending a quote/bracket\n",
    "    s3 = s2\n",
    "    if s3.count(\"'\") % 2 == 1:\n",
    "        # append a closing single-quote and bracket if needed\n",
    "        if not s3.strip().endswith(\"'\"):\n",
    "            s3 = s3 + \"'\"\n",
    "        if s3.strip().startswith(\"[\") and not s3.strip().endswith(\"]\"):\n",
    "            s3 = s3 + \"]\"\n",
    "    try:\n",
    "        val = ast.literal_eval(s3)\n",
    "        return list(val) if isinstance(val, (list, tuple)) else val\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 7) FINAL TOLERANT FALLBACK: extract comma-separated tokens, honoring quoted substrings\n",
    "    # This will capture 'name', \"name\", or unquoted tokens between commas/brackets\n",
    "    tokens = []\n",
    "    for m in re.finditer(r\"\"\"\n",
    "            '([^']*)'       |   # single-quoted token\n",
    "            \"([^\"]*)\"       |   # double-quoted token\n",
    "            ([^\\[\\],]+)         # unquoted token (stop at comma or bracket)\n",
    "        \"\"\", orig, re.VERBOSE):\n",
    "        # pick the non-empty capture\n",
    "        token = m.group(1) or m.group(2) or m.group(3)\n",
    "        token = token.strip()\n",
    "        if token:\n",
    "            tokens.append(token)\n",
    "    # If still empty, return empty list rather than None\n",
    "    return tokens\n",
    "\n",
    "# Apply to dataframe and record which indices needed repair\n",
    "problematic_indices = []\n",
    "repair_method = {}\n",
    "\n",
    "for idx, val in tqdm(df['researchers'].items(), total=len(df)):\n",
    "    parsed = safe_parse_researchers(val)\n",
    "    # ensure it's a list (if single string, wrap it)\n",
    "    if parsed is None:\n",
    "        parsed = []\n",
    "    if isinstance(parsed, (str, int, float)):\n",
    "        parsed = [parsed]\n",
    "    df.at[idx, 'researchers'] = parsed\n",
    "\n",
    "# Optionally, find rows that are still empty or suspicious after parsing\n",
    "still_bad = df[df['researchers'].apply(lambda x: (not isinstance(x, list)) or any(pd.isna(i) for i in x) )].index.tolist()\n",
    "print(f\"Total rows parsed: {len(df)}\")\n",
    "print(f\"Rows still suspicious after repair: {len(still_bad)} (examples: {still_bad[:20]})\")\n",
    "\n",
    "df['researcher_count'] = df['researchers'].progress_apply(lambda x: len(x) if str(x)!= '[]' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29bbfe9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10846/10846 [00:03<00:00, 2752.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows parsed: 10846\n",
      "Rows still suspicious after repair: 0 (examples: [])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Apply to dataframe and record which indices needed repair\n",
    "problematic_indices = []\n",
    "repair_method = {}\n",
    "\n",
    "for idx, val in tqdm(df['research_orgs'].items(), total=len(df)):\n",
    "    parsed = safe_parse_researchers(val)\n",
    "    # ensure it's a list (if single string, wrap it)\n",
    "    if parsed is None:\n",
    "        parsed = []\n",
    "    if isinstance(parsed, (str, int, float)):\n",
    "        parsed = [parsed]\n",
    "    df.at[idx, 'research_orgs'] = parsed\n",
    "\n",
    "# Optionally, find rows that are still empty or suspicious after parsing\n",
    "still_bad = df[df['research_orgs'].apply(lambda x: (not isinstance(x, list)) or any(pd.isna(i) for i in x) )].index.tolist()\n",
    "print(f\"Total rows parsed: {len(df)}\")\n",
    "print(f\"Rows still suspicious after repair: {len(still_bad)} (examples: {still_bad[:20]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3272a04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample researcher entry:\n",
      "{'first_name': 'Xian-Wen', 'id': 'ur.012214661411.72', 'last_name': 'Shang', 'orcid_id': ['0000-0001-6143-9349', '0000-0002-2362-3222'], 'research_orgs': ['grid.43169.39', 'grid.12981.33', 'grid.415105.4', 'grid.410670.4', 'grid.284723.8', 'grid.1008.9', 'grid.416153.4', 'grid.16890.36', 'grid.508448.5', 'grid.411958.0', 'grid.1103439.3d', 'grid.506261.6', 'grid.198530.6', 'grid.418002.f', 'grid.413352.2']}\n",
      "\n",
      "Data type: <class 'list'>\n",
      "Number of researchers in first paper: 12\n"
     ]
    }
   ],
   "source": [
    "# Examine the data structure\n",
    "print(\"Sample researcher entry:\")\n",
    "print(df['researchers'][0][0])\n",
    "print(\"\\nData type:\", type(df['researchers'][0]))\n",
    "print(\"Number of researchers in first paper:\", len(df['researchers'][0]) if isinstance(df['researchers'][0], list) else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec3f8ff",
   "metadata": {},
   "source": [
    "# Research Plan: UK Biobank Network Analysis Over Time\n",
    "\n",
    "## Data Structure Update\n",
    "The `researchers` column contains a **list of dictionaries** with the following structure:\n",
    "- `id`: Unique researcher identifier (e.g., 'ur.012214661411.72')\n",
    "- `first_name`: Researcher's first name\n",
    "- `last_name`: Researcher's last name\n",
    "- `orcid_id`: List of ORCID identifiers\n",
    "- `research_orgs`: List of organization IDs associated with this researcher\n",
    "\n",
    "Similarly, `research_orgs` likely contains organization information.\n",
    "\n",
    "## Network Analysis Approach\n",
    "We'll build **co-authorship networks** where:\n",
    "- **Nodes** = researchers (identified by their `id`)\n",
    "- **Edges** = collaboration (researchers who co-authored papers)\n",
    "\n",
    "## Key Metrics to Track Over Time:\n",
    "- Total unique authors\n",
    "- Total collaborations (edges)\n",
    "- Giant component size (largest connected group)\n",
    "- Number of isolates (single-author papers)\n",
    "- Number of connected components\n",
    "- Network density & clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "138d2834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NetworkX version: 3.6\n",
      "Number of papers: 10846\n",
      "Year range: 2013 to 2025\n"
     ]
    }
   ],
   "source": [
    "# Install/import required packages\n",
    "import networkx as nx\n",
    "from scipy import sparse as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "print(\"NetworkX version:\", nx.__version__)\n",
    "print(\"Number of papers:\", len(df))\n",
    "print(\"Year range:\", df['year'].min(), \"to\", df['year'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dab68e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_author_network(df_subset):\n",
    "    \"\"\"\n",
    "    Build author-author co-authorship network from papers.\n",
    "    Each researcher is identified by their 'id' field in the researcher dictionary.\n",
    "    \n",
    "    Based on PopStudies approach: create bipartite author-paper matrix,\n",
    "    then project to author-author network where edges represent co-authorship.\n",
    "    \"\"\"\n",
    "    # Filter papers with valid researchers\n",
    "    df_valid = df_subset[df_subset['researchers'].apply(\n",
    "        lambda x: isinstance(x, list) and len(x) > 0)].copy()\n",
    "    \n",
    "    if len(df_valid) == 0:\n",
    "        return None, {}\n",
    "    \n",
    "    # Extract unique papers and researchers\n",
    "    unique_papers = df_valid['id'].unique().tolist()\n",
    "    unique_authors = set()\n",
    "    \n",
    "    # Build author info dictionary: author_id -> researcher dict\n",
    "    author_info = {}\n",
    "    \n",
    "    for researchers_list in df_valid['researchers']:\n",
    "        if isinstance(researchers_list, list):\n",
    "            for researcher in researchers_list:\n",
    "                if isinstance(researcher, dict) and 'id' in researcher:\n",
    "                    author_id = researcher['id']\n",
    "                    unique_authors.add(author_id)\n",
    "                    if author_id not in author_info:\n",
    "                        author_info[author_id] = researcher\n",
    "    \n",
    "    unique_authors = list(unique_authors)\n",
    "    \n",
    "    if len(unique_authors) == 0:\n",
    "        return None, {}\n",
    "    \n",
    "    # Create integer mappings for matrix\n",
    "    paper_to_int = {paper: idx for idx, paper in enumerate(unique_papers)}\n",
    "    author_to_int = {author: idx for idx, author in enumerate(unique_authors)}\n",
    "    int_to_author = {idx: author for author, idx in author_to_int.items()}\n",
    "    \n",
    "    # Build author-paper tuples\n",
    "    author_paper_tuples = []\n",
    "    for _, row in df_valid.iterrows():\n",
    "        paper_id = paper_to_int[row['id']]\n",
    "        researchers_list = row['researchers']\n",
    "        \n",
    "        if isinstance(researchers_list, list):\n",
    "            for researcher in researchers_list:\n",
    "                if isinstance(researcher, dict) and 'id' in researcher:\n",
    "                    author_id = researcher['id']\n",
    "                    author_idx = author_to_int[author_id]\n",
    "                    author_paper_tuples.append((author_idx, paper_id))\n",
    "    \n",
    "    if len(author_paper_tuples) == 0:\n",
    "        return None, {}\n",
    "    \n",
    "    # Create sparse bipartite author-paper matrix\n",
    "    n_authors = len(unique_authors)\n",
    "    n_papers = len(unique_papers)\n",
    "    \n",
    "    author_ids, paper_ids = zip(*author_paper_tuples)\n",
    "    AP = sp.csc_matrix(\n",
    "        (np.ones(len(author_paper_tuples)), (author_ids, paper_ids)),\n",
    "        shape=(n_authors, n_papers)\n",
    "    )\n",
    "    \n",
    "    # Project to author-author network: AA = AP * AP^T\n",
    "    # This creates edges between authors who co-authored papers\n",
    "    AA = AP.dot(AP.T)\n",
    "    \n",
    "    # Remove self-loops (diagonal elements)\n",
    "    AA = AA - sp.diags(AA.diagonal())\n",
    "    \n",
    "    # Convert to NetworkX graph\n",
    "    G = nx.from_scipy_sparse_array(AA)\n",
    "    \n",
    "    # Return both graph and mapping with author info\n",
    "    author_mapping = {\n",
    "        'int_to_author': int_to_author,\n",
    "        'author_to_int': author_to_int,\n",
    "        'author_info': author_info\n",
    "    }\n",
    "    \n",
    "    return G, author_mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22afbe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_network_metrics(G):\n",
    "    \"\"\"\n",
    "    Compute comprehensive network metrics including tie strength and structural properties.\n",
    "    Returns a dictionary with extensive network statistics.\n",
    "    \"\"\"\n",
    "    if G is None or G.number_of_nodes() == 0:\n",
    "        return {\n",
    "            'n_nodes': 0,\n",
    "            'n_edges': 0,\n",
    "            'density': 0,\n",
    "            'n_components': 0,\n",
    "            'n_isolates': 0,\n",
    "            'giant_nodes': 0,\n",
    "            'giant_edges': 0,\n",
    "            'giant_density': 0,\n",
    "            'avg_clustering': 0,\n",
    "            'avg_degree': 0,\n",
    "            'max_degree': 0,\n",
    "            'avg_tie_strength': 0,\n",
    "            'max_tie_strength': 0,\n",
    "            'median_tie_strength': 0,\n",
    "            'transitivity': 0,\n",
    "            'assortativity': 0,\n",
    "            'diameter': 0,\n",
    "            'avg_shortest_path': 0,\n",
    "            'n_weak_ties': 0,\n",
    "            'n_strong_ties': 0,\n",
    "            'weak_tie_ratio': 0\n",
    "        }\n",
    "    \n",
    "    # ============ Basic metrics ============\n",
    "    n_nodes = G.number_of_nodes()\n",
    "    n_edges = G.number_of_edges()\n",
    "    density = nx.density(G)\n",
    "    \n",
    "    # ============ Degree distribution ============\n",
    "    degrees = [d for n, d in G.degree()]\n",
    "    avg_degree = np.mean(degrees) if len(degrees) > 0 else 0\n",
    "    max_degree = max(degrees) if len(degrees) > 0 else 0\n",
    "    \n",
    "    # ============ Connected components ============\n",
    "    components = list(nx.connected_components(G))\n",
    "    n_components = len(components)\n",
    "    \n",
    "    # Isolates (nodes with degree 0)\n",
    "    isolates = list(nx.isolates(G))\n",
    "    n_isolates = len(isolates)\n",
    "    \n",
    "    # Giant component (largest connected component)\n",
    "    if n_components > 0:\n",
    "        giant_component = max(components, key=len)\n",
    "        G_giant = G.subgraph(giant_component)\n",
    "        giant_nodes = G_giant.number_of_nodes()\n",
    "        giant_edges = G_giant.number_of_edges()\n",
    "        giant_density = nx.density(G_giant) if giant_nodes > 1 else 0\n",
    "    else:\n",
    "        giant_nodes = giant_edges = giant_density = 0\n",
    "        G_giant = None\n",
    "    \n",
    "    # ============ Clustering ============\n",
    "    \"\"\" \n",
    "    try:\n",
    "        avg_clustering = nx.average_clustering(G)\n",
    "        transitivity = nx.transitivity(G)  # Global clustering coefficient\n",
    "    except:\n",
    "        avg_clustering = 0\n",
    "        transitivity = 0\n",
    "    \"\"\"\n",
    "    # ============ Tie Strength Analysis ============\n",
    "    # Tie strength = edge weight (number of co-authored papers)\n",
    "    tie_strengths = []\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        weight = data.get('weight', 1)  # Default weight is 1\n",
    "        tie_strengths.append(weight)\n",
    "    \n",
    "    if len(tie_strengths) > 0:\n",
    "        avg_tie_strength = np.mean(tie_strengths)\n",
    "        max_tie_strength = max(tie_strengths)\n",
    "        median_tie_strength = np.median(tie_strengths)\n",
    "        \n",
    "        # Define weak vs strong ties (threshold: median or mean)\n",
    "        threshold = median_tie_strength\n",
    "        n_weak_ties = sum(1 for w in tie_strengths if w <= threshold)\n",
    "        n_strong_ties = sum(1 for w in tie_strengths if w > threshold)\n",
    "        weak_tie_ratio = n_weak_ties / len(tie_strengths) if len(tie_strengths) > 0 else 0\n",
    "    else:\n",
    "        avg_tie_strength = max_tie_strength = median_tie_strength = 0\n",
    "        n_weak_ties = n_strong_ties = weak_tie_ratio = 0\n",
    "    \n",
    "    # ============ Assortativity (degree correlation) ============\n",
    "    \"\"\" \n",
    "    try:\n",
    "        assortativity = nx.degree_assortativity_coefficient(G)\n",
    "    except:\n",
    "        assortativity = 0\n",
    "    \"\"\"\n",
    "    # ============ Path-based metrics (only for giant component if network is disconnected) ============\n",
    "    # COMMENTED OUT: These are computationally expensive O(n²) operations\n",
    "    \"\"\"\n",
    "    diameter = 0\n",
    "    avg_shortest_path = 0\n",
    "    \n",
    "    if G_giant is not None and giant_nodes > 1:\n",
    "        try:\n",
    "            # Diameter (longest shortest path in giant component)\n",
    "            diameter = nx.diameter(G_giant)\n",
    "        except:\n",
    "            diameter = 0\n",
    "        \n",
    "        try:\n",
    "            # Average shortest path length (giant component)\n",
    "            avg_shortest_path = nx.average_shortest_path_length(G_giant)\n",
    "        except:\n",
    "            avg_shortest_path = 0\n",
    "    \"\"\"\n",
    "    # Set to 0 when commented out\n",
    "    diameter = 0\n",
    "    avg_shortest_path = 0\n",
    "    \n",
    "    return {\n",
    "        # Basic structure\n",
    "        'n_nodes': n_nodes,\n",
    "        'n_edges': n_edges,\n",
    "        'density': density,\n",
    "        \n",
    "        # Components\n",
    "        'n_components': n_components,\n",
    "        'n_isolates': n_isolates,\n",
    "        'giant_nodes': giant_nodes,\n",
    "        'giant_edges': giant_edges,\n",
    "        'giant_density': giant_density,\n",
    "        \n",
    "        # Degree metrics\n",
    "        'avg_degree': avg_degree,\n",
    "        'max_degree': max_degree,\n",
    "        \n",
    "        # Clustering\n",
    "        #'avg_clustering': avg_clustering,\n",
    "        #'transitivity': transitivity,\n",
    "        \n",
    "        # Tie strength\n",
    "        'avg_tie_strength': avg_tie_strength,\n",
    "        'max_tie_strength': max_tie_strength,\n",
    "        'median_tie_strength': median_tie_strength,\n",
    "        'n_weak_ties': n_weak_ties,\n",
    "        'n_strong_ties': n_strong_ties,\n",
    "        'weak_tie_ratio': weak_tie_ratio,\n",
    "        \n",
    "        # Network structure\n",
    "        #'assortativity': assortativity,\n",
    "        'diameter': diameter,\n",
    "        'avg_shortest_path': avg_shortest_path\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88110c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing network evolution from 2013 to 2025\n",
      "Total papers: 10846\n",
      "Papers with researchers: 10801\n",
      "\n",
      "Building temporal networks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:18<00:00,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Network metrics computed successfully!\n",
      "============================================================\n",
      "      n_nodes  n_edges   density  n_components  n_isolates  giant_nodes  \\\n",
      "year                                                                      \n",
      "2013       31      132  0.283871             4           0           14   \n",
      "2014       64      280  0.138889             7           0           18   \n",
      "2015      172     1136  0.077247            16           0           35   \n",
      "2016      792    11447  0.036544            14           0          644   \n",
      "2017     1685    24657  0.017379            29           1         1526   \n",
      "2018     3244    51654  0.009820            37           3         2986   \n",
      "2019     5224    82974  0.006082            60           4         4808   \n",
      "2020     8683   141031  0.003742           111           4         7897   \n",
      "2021    13069   211437  0.002476           155           6        12000   \n",
      "2022    18024   293095  0.001805           201           9        16688   \n",
      "\n",
      "      giant_edges  giant_density  avg_degree  max_degree  avg_tie_strength  \\\n",
      "year                                                                         \n",
      "2013           91       1.000000    8.516129          13          1.689394   \n",
      "2014          119       0.777778    8.750000          17          2.217857   \n",
      "2015          595       1.000000   13.209302          34          1.460387   \n",
      "2016        10420       0.050327   28.906566         144          1.216214   \n",
      "2017        24160       0.020764   29.266469         172          1.204404   \n",
      "2018        50618       0.011358   31.845869         286          1.245209   \n",
      "2019        81299       0.007035   31.766462         400          1.298563   \n",
      "2020       138004       0.004426   32.484395         580          1.292617   \n",
      "2021       206984       0.002875   32.357028         685          1.312433   \n",
      "2022       288015       0.002069   32.522747         784          1.330364   \n",
      "\n",
      "      max_tie_strength  median_tie_strength  n_weak_ties  n_strong_ties  \\\n",
      "year                                                                      \n",
      "2013               2.0                  2.0          132              0   \n",
      "2014               6.0                  1.0          147            133   \n",
      "2015               9.0                  1.0          952            184   \n",
      "2016              20.0                  1.0        10366           1081   \n",
      "2017              31.0                  1.0        22417           2240   \n",
      "2018              39.0                  1.0        45202           6452   \n",
      "2019              53.0                  1.0        71020          11954   \n",
      "2020              63.0                  1.0       121454          19577   \n",
      "2021              79.0                  1.0       180509          30928   \n",
      "2022              93.0                  1.0       249507          43588   \n",
      "\n",
      "      weak_tie_ratio  diameter  avg_shortest_path  n_papers  \n",
      "year                                                         \n",
      "2013        1.000000         0                  0         5  \n",
      "2014        0.525000         0                  0        19  \n",
      "2015        0.838028         0                  0        49  \n",
      "2016        0.905565         0                  0       145  \n",
      "2017        0.909154         0                  0       312  \n",
      "2018        0.875092         0                  0       645  \n",
      "2019        0.855931         0                  0      1158  \n",
      "2020        0.861187         0                  0      1978  \n",
      "2021        0.853725         0                  0      3191  \n",
      "2022        0.851284         0                  0      4674  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Build temporal networks: cumulative year-by-year analysis\n",
    "# This tracks how the network grows over time\n",
    "\n",
    "# Ensure year is numeric and sort\n",
    "df_clean = df[df['year'].notna()].copy()\n",
    "df_clean['year'] = df_clean['year'].astype(int)\n",
    "df_clean = df_clean.sort_values('year')\n",
    "\n",
    "# Get year range\n",
    "min_year = int(df_clean['year'].min())\n",
    "max_year = int(df_clean['year'].max())\n",
    "\n",
    "print(f\"Analyzing network evolution from {min_year} to {max_year}\")\n",
    "print(f\"Total papers: {len(df_clean)}\")\n",
    "print(f\"Papers with researchers: {len(df_clean[df_clean['researcher_count'] > 0])}\")\n",
    "\n",
    "# Initialize results storage\n",
    "years = range(min_year, max_year + 1)\n",
    "metrics_over_time = []\n",
    "\n",
    "\n",
    "# Build cumulative networks year by year\n",
    "print(\"\\nBuilding temporal networks...\")\n",
    "for year in tqdm(years):\n",
    "    # Cumulative: all papers up to and including this year\n",
    "    df_upto_year = df_clean[df_clean['year'] <= year]\n",
    "    \n",
    "    # Build network\n",
    "    G, author_mapping = build_author_network(df_upto_year)\n",
    "    \n",
    "    # Compute metrics\n",
    "    metrics = compute_network_metrics(G)\n",
    "    metrics['year'] = year\n",
    "    metrics['n_papers'] = len(df_upto_year)\n",
    "    \n",
    "    metrics_over_time.append(metrics)\n",
    "\n",
    "# Convert to dataframe\n",
    "network_metrics_df = pd.DataFrame(metrics_over_time)\n",
    "network_metrics_df = network_metrics_df.set_index('year')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Network metrics computed successfully!\")\n",
    "print(\"=\"*60)\n",
    "print(network_metrics_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf233172",
   "metadata": {},
   "source": [
    "## Part 2: Cumulative Network Evolution & Animation\n",
    "\n",
    "Now we'll create visualizations showing how the **cumulative network grows over time** - each year includes all papers published up to that year. We'll generate an animated GIF showing network evolution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf9c369",
   "metadata": {},
   "source": [
    "## Enhanced GIF Animation with Metrics Panel\n",
    "\n",
    "Create an animated GIF showing:\n",
    "- **Left**: Network visualization (cumulative, showing giant component & isolates)\n",
    "- **Right**: 3 metric plots vertically stacked:\n",
    "  1. Network nodes and edges\n",
    "  2. Average collaborators per author\n",
    "  3. Giant component growth\n",
    "\n",
    "Each frame shows a **sliding time window** (e.g., past X years) to keep the right panel focused and readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b426d275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Time window: 12 years\n",
      "  Frames directory: ../data/network_frames_with_metrics\n"
     ]
    }
   ],
   "source": [
    "# Configuration for enhanced GIF with metrics panel\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Time window for x-axis (years to show in metrics plots)\n",
    "TIME_WINDOW_YEARS = 12  # Show past 12 years in each frame\n",
    "\n",
    "# Create directory for enhanced frames\n",
    "enhanced_frames_dir = '../data/network_frames_with_metrics'\n",
    "os.makedirs(enhanced_frames_dir, exist_ok=True)\n",
    "min_year = 2015 \n",
    "max_year = 2025\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Time window: {TIME_WINDOW_YEARS} years\")\n",
    "print(f\"  Frames directory: {enhanced_frames_dir}\")\n",
    "\n",
    "plt.rcParams['font.family'] = 'Helvetica'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5fa97797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating enhanced network snapshots with metrics panels...\n",
      "This may take several minutes for large networks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saving frame for year 2015 to ../data/network_frames_with_metrics/network_enhanced_2015.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 10/11 [58:27<13:34, 814.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saving frame for year 2025 to ../data/network_frames_with_metrics/network_enhanced_2025.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [1:37:51<00:00, 533.73s/it] \n"
     ]
    }
   ],
   "source": [
    "# Generate enhanced frames: Network (left) + Metrics (right)\n",
    "\n",
    "print(\"Generating enhanced network snapshots with metrics panels...\")\n",
    "print(\"This may take several minutes for large networks...\")\n",
    "\n",
    "# Select years to visualize (every year)\n",
    "years_to_plot = list(range(min_year, max_year + 1, 1))\n",
    "if max_year not in years_to_plot:\n",
    "    years_to_plot.append(max_year)\n",
    "\n",
    "enhanced_frame_files = []\n",
    "\n",
    "for year in tqdm(years_to_plot):\n",
    "    # Build CUMULATIVE network up to this year\n",
    "    df_upto_year = df_clean[df_clean['year'] <= year]\n",
    "    G, author_mapping = build_author_network(df_upto_year)\n",
    "    \n",
    "    if G is None or G.number_of_nodes() == 0:\n",
    "        continue\n",
    "    \n",
    "    # Create figure with GridSpec: left for network, right for metrics\n",
    "    fig = plt.figure(figsize=(16,9))\n",
    "    gs = fig.add_gridspec(4, 2, width_ratios=[2, 1], hspace=0.35, wspace=0.3,\n",
    "                          left=0.05, right=0.97, top=0.93, bottom=0.07)\n",
    "    \n",
    "    # ========== LEFT: Network Visualization ==========\n",
    "    ax_network = fig.add_subplot(gs[:, 0])\n",
    "    \n",
    "    # Get components for coloring\n",
    "    components = list(nx.connected_components(G))\n",
    "    components_sorted = sorted(components, key=len, reverse=True)\n",
    "    \n",
    "    # Color scheme\n",
    "    \n",
    "    colors_scheme = [ \"#B80C09\", \"#D4AF37\", \"#6E8B3D\",\"#345995\"]\n",
    "        \n",
    "    # Assign colors and sizes based on component type\n",
    "    node_colors = []\n",
    "    node_sizes = []\n",
    "    \n",
    "    for node in G.nodes():\n",
    "        component_idx = None\n",
    "        for idx, comp in enumerate(components_sorted):\n",
    "            if node in comp:\n",
    "                component_idx = idx\n",
    "                break\n",
    "        \n",
    "        comp_size = len(components_sorted[component_idx]) if component_idx is not None else 1\n",
    "        \n",
    "        if comp_size == 1:  # Isolates\n",
    "            node_colors.append(colors_scheme[0])\n",
    "            node_sizes.append(20)\n",
    "        elif comp_size <= 5:\n",
    "            node_colors.append(colors_scheme[1])\n",
    "            node_sizes.append(30)\n",
    "        elif component_idx == 0:  # Giant component\n",
    "            node_colors.append(colors_scheme[3])\n",
    "            node_sizes.append(35)\n",
    "        else:\n",
    "            node_colors.append(colors_scheme[2])\n",
    "            node_sizes.append(25)\n",
    "    \n",
    "    # Compute layout\n",
    "    iterations = 20 if G.number_of_nodes() > 1000 else 40\n",
    "    pos = nx.spring_layout(G, k=0.3, iterations=iterations, seed=48652)\n",
    "    \n",
    "    # Draw network\n",
    "    nx.draw_networkx_nodes(G, pos, \n",
    "                          node_color=node_colors, \n",
    "                          node_size=node_sizes,\n",
    "                          alpha=0.7,\n",
    "                          edgecolors='k',\n",
    "                          linewidths=0.5,\n",
    "                          ax=ax_network)\n",
    "    \n",
    "    nx.draw_networkx_edges(G, pos, \n",
    "                          alpha=0.15, \n",
    "                          width=0.3,\n",
    "                          ax=ax_network)\n",
    "    \n",
    "    # Network title - positioned at bottom\n",
    "    metrics_current = network_metrics_df.loc[year]\n",
    "    title_text = (f'UK Biobank Co-authorship Network (Through {year})\\n' + \n",
    "                  f'{metrics_current[\"n_nodes\"]:,.0f} authors • ' +\n",
    "                  f'{metrics_current[\"n_edges\"]:,.0f} collaborations • ' +\n",
    "                  f'Giant Component: {metrics_current[\"giant_nodes\"]:,.0f} ' +\n",
    "                  f'({100*metrics_current[\"giant_nodes\"]/metrics_current[\"n_nodes\"]:.1f}%)')\n",
    "    \n",
    "    ax_network.text(0.5, 0.01, title_text, transform=ax_network.transAxes,\n",
    "                    fontsize=13, fontweight='bold', ha='center', va='bottom')\n",
    "                    #bbox=dict(boxstyle='round,pad=0.3', fc='white', ec='k', lw=1))\n",
    "    \n",
    "    ax_network.axis('off')\n",
    "    \n",
    "    # Add legend\n",
    "    legend_elements = [\n",
    "        mlines.Line2D([], [], color=colors_scheme[0], marker='o', linestyle='None',\n",
    "                     markersize=10, label='Isolates', markeredgecolor='k', markeredgewidth=0.5, alpha=0.7),\n",
    "        # mark on other colors too \n",
    "        mlines.Line2D([], [], color=colors_scheme[1], marker='o', linestyle='None',\n",
    "                     markersize=10, label='2-5 Small Collaborations', markeredgecolor='k', markeredgewidth=0.5, alpha=0.7),\n",
    "        mlines.Line2D([], [], color=colors_scheme[2], marker='o', linestyle='None',\n",
    "                     markersize=10, label='Other Medium Collaborations', markeredgecolor='k', markeredgewidth=0.5, alpha=0.7),\n",
    "        mlines.Line2D([], [], color=colors_scheme[3], marker='o', linestyle='None',\n",
    "                     markersize=10, label='Giant Component', markeredgecolor='k', markeredgewidth=0.5, alpha=0.7)\n",
    "    ]\n",
    "    ax_network.legend(handles=legend_elements,bbox_to_anchor=(0.85, 0.1), frameon=True, \n",
    "                     framealpha=0.95, edgecolor='k', fontsize=11)\n",
    "    \n",
    "    # ========== RIGHT: Metrics Panels (3 stacked vertically) ==========\n",
    "    \n",
    "    # Determine time window for x-axis\n",
    "    x_min = max(min_year, year - TIME_WINDOW_YEARS + 1)\n",
    "    x_max = max_year  # Always show full x-axis to max_year\n",
    "    \n",
    "    # Filter data for this time window (up to current year)\n",
    "    data_mask = (network_metrics_df.index >= x_min) & (network_metrics_df.index <= year)\n",
    "    data_window = network_metrics_df[data_mask]\n",
    "    \n",
    "\n",
    "    # Panel 0: Nodes \n",
    "    ax0 = fig.add_subplot(gs[0, 1])\n",
    "    \n",
    "    ax0.plot(data_window.index, data_window['n_nodes'], \n",
    "             color=colors_scheme[2], linewidth=2.5, marker='o', markersize=4, label='Authors')\n",
    "    ax0.fill_between(data_window.index, 0, data_window['n_nodes'], alpha=0.3, color=colors_scheme[2])\n",
    "    \n",
    "    #ax0.set_ylabel('Authors', fontsize=13, fontweight='bold')\n",
    "    ax0.tick_params(labelsize=9)\n",
    "\n",
    "    ax0.set_ylim(-5,40000)\n",
    "    ax0.set_xlim(x_min-0.5, x_max+1)\n",
    "    ax0.set_title('Network Growth - Authors', fontsize=12, fontweight='bold', loc='left', pad=8)\n",
    "    ax0.grid(alpha=0.3, linestyle='--', linewidth=0.5)\n",
    "    ax0.tick_params(axis='x', labelsize=9)\n",
    "    ax0.set_xlabel('')\n",
    "    \n",
    "    \n",
    "    # Panel 1: Nodes and Edges\n",
    "    ax1 = fig.add_subplot(gs[1, 1])\n",
    "    \n",
    "    ax1.plot(data_window.index, data_window['n_edges'], \n",
    "                  color=\"#B80C09\", linewidth=2.5, marker='o', markersize=4, label='Collaborations')\n",
    "    ax1.fill_between(data_window.index, 0, data_window['n_edges'], \n",
    "                     alpha=0.3, color=\"#B80C09\")\n",
    "    #ax1.set_ylabel('Collaborations', fontsize=13, fontweight='bold')\n",
    "    ax1.set_ylim(-5,580000)\n",
    "    ax1.tick_params(labelsize=9)\n",
    "    ax1.set_xlim(x_min-0.5, x_max+1)\n",
    "    ax1.set_title('Network Growth - Collaborations', fontsize=12, fontweight='bold', loc='left', pad=8)\n",
    "    ax1.grid(alpha=0.3, linestyle='--', linewidth=0.5)\n",
    "    ax1.tick_params(axis='x', labelsize=9)\n",
    "    ax1.set_xlabel('')\n",
    "    \n",
    "    # Panel 2: Average Degree (Collaborators per Author)\n",
    "    ax2 = fig.add_subplot(gs[2, 1])\n",
    "    \n",
    "    ax2.plot(data_window.index, data_window['avg_degree'], \n",
    "             color= \"#D4AF37\", linewidth=2.5, marker='o', markersize=4)\n",
    "    ax2.fill_between(data_window.index, 0, data_window['avg_degree'],\n",
    "                     alpha=0.3, color= \"#D4AF37\")\n",
    "    \n",
    "    #ax2.set_ylabel('Avg Degree', fontsize=13, fontweight='bold')\n",
    "    ax2.set_xlim(x_min-0.5,x_max+1)\n",
    "    ax2.set_title('Avg Collaborators per Author', fontsize=12, fontweight='bold', loc='left', pad=8)\n",
    "    ax2.grid(alpha=0.3, linestyle='--', linewidth=0.5)\n",
    "    ax2.tick_params(labelsize=9)\n",
    "    ax2.set_ylim(0,33)\n",
    "    ax2.set_xlabel('')\n",
    "    \n",
    "    # Panel 3: Giant Component Growth\n",
    "    ax3 = fig.add_subplot(gs[3, 1])\n",
    "    \n",
    "    ax3.plot(data_window.index, data_window['giant_nodes'], \n",
    "             color=colors_scheme[3], linewidth=2.5, marker='o', markersize=4)\n",
    "    ax3.fill_between(data_window.index, 0, data_window['giant_nodes'],\n",
    "                     alpha=0.3, color=colors_scheme[3])\n",
    "    \n",
    "    #ax3.set_ylabel('Giant Component', fontsize=13, fontweight='bold')\n",
    "    ax3.set_xlabel('Year', fontsize=13, fontweight='bold')\n",
    "    ax3.set_xlim(x_min-0.5, x_max+1)\n",
    "    ax3.set_title('Giant Component Size', fontsize=12, fontweight='bold', loc='left', pad=8)\n",
    "    ax3.grid(alpha=0.3, linestyle='--', linewidth=0.5)\n",
    "    ax3.tick_params(labelsize=9)\n",
    "    ax3.set_ylim(-5,35000)\n",
    "    \n",
    "    # Apply styling\n",
    "    sns.despine(ax=ax1)\n",
    "    sns.despine(ax=ax0)\n",
    "    sns.despine(ax=ax2)\n",
    "    sns.despine(ax=ax3)\n",
    "\n",
    "    \n",
    "    # Save frame\n",
    "    frame_file = os.path.join(enhanced_frames_dir, f'network_enhanced_{year}.png')\n",
    "    if year in [min_year, max_year]:\n",
    "        print(f\" Saving frame for year {year} to {frame_file}\")\n",
    "        plt.savefig(os.path.join(enhanced_frames_dir, f'network_enhanced_{year}.svg'), bbox_inches='tight', dpi=300, facecolor='white')\n",
    "        \n",
    "    plt.savefig(frame_file, bbox_inches='tight', dpi=300, facecolor='white')\n",
    "    plt.close()\n",
    "    \n",
    "    enhanced_frame_files.append(frame_file)\n",
    "  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72deaa27",
   "metadata": {},
   "outputs": [],
   "source": [
    "components = list(nx.connected_components(G))\n",
    "components_sorted = sorted(components, key=len, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5242d376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component 1: Size 35\n",
      "Component 2: Size 28\n",
      "Component 3: Size 25\n",
      "Component 4: Size 16\n",
      "Component 5: Size 14\n",
      "Component 6: Size 11\n",
      "Component 7: Size 10\n",
      "Component 8: Size 6\n",
      "Component 9: Size 5\n",
      "Component 10: Size 5\n",
      "Component 11: Size 4\n",
      "Component 12: Size 4\n",
      "Component 13: Size 3\n",
      "Component 14: Size 2\n",
      "Component 15: Size 2\n",
      "Component 16: Size 2\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(components_sorted)):\n",
    "    print(f\"Component {i+1}: Size {len(components_sorted[i])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8f9f895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(components_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "450a582a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating enhanced animated GIF with metrics panel...\n"
     ]
    }
   ],
   "source": [
    "# Create enhanced animated GIF\n",
    "\n",
    "print(\"Creating enhanced animated GIF with metrics panel...\")\n",
    "\n",
    "# Load all enhanced frames\n",
    "enhanced_frames = []\n",
    "for frame_file in enhanced_frame_files:\n",
    "    img = Image.open(frame_file)\n",
    "    enhanced_frames.append(img)\n",
    "\n",
    "# Save as GIF\n",
    "enhanced_gif_path = '../fig/network/network_evolution_with_metrics.gif'\n",
    "enhanced_frames[0].save(\n",
    "    enhanced_gif_path,\n",
    "    save_all=True,\n",
    "    append_images=enhanced_frames[1:],\n",
    "    duration=2000,  # milliseconds per frame \n",
    "    loop=0,\n",
    "    optimize=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb78271",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robustipy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
