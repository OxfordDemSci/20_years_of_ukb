{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d76a5934-b998-49c3-be96-acc15f4c8b24",
   "metadata": {},
   "source": [
    "### Cleaning UKBB Showcase Data and Merging onto Dimensions\n",
    "##### Last Updated: 31.03.2025\n",
    "\n",
    "Note: a large number of helper functions are stored in `./src/helpers_scrape.py`. Launch this notebook from `./src/`. See other notebooks for visualisations of this and related data (from other DSL calls and a bespoke dataset).\n",
    "\n",
    "First, lets load in a couple of things we'll need; a couple are standard python libraries, but most are our own custom functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7a98c55-f5d6-4b83-a863-8142b5cfb3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from helpers_scrape import (get_ukb_showcase_data, # grab the upstream UKB file\n",
    "                            login_dimcli, # simple login to DIMCLI helper function\n",
    "                            get_raw_data, # grab raw article level data from the DSL\n",
    "                            merger, # merge individual API returns together\n",
    "                            evaluate_raw_scrape, # evaluate the original scrape of raw data\n",
    "                            make_long_refs, # explode our original scrape to be long for refs\n",
    "                            save_combined # save all wrangled files out\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faac8772-10d5-4347-9e31-a5f0488f5c24",
   "metadata": {},
   "source": [
    "We want our analysis to be easily updateable, but not so frequently updateable that it puts pressure on the API or makes our results change every time we run the script. So, lets timestamp our incoming data into months,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1c1bab3-1369-4fc8-8638-292ceb96a2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y%m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775ea64d-e2c5-42c7-bc1e-24714a1fffbe",
   "metadata": {},
   "source": [
    "Next, lets grab the raw UKBB data from upstream at the UKBB showcase. We're going to clean a couple of fields, and also look to see if there's anything wonky going on within it at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "446cbdba-e562-49c8-973d-0c766becc2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records which we start with:  8553\n",
      "Number of duplicated DOIS (excl. NaN):  41\n",
      "Duplicate DOIs saved: ../data/raw/ukb_data_duplicated_DOIs.txt\n",
      "Note: looks like one DOI occurs 3x...\n",
      "Dropping all but the first DOIs (21) records.\n",
      "Number of duplicated pmid (excl. NaN and pmid==0):  0\n",
      "Number of duplicated titles (inc. NaN):  59\n",
      "Duplicate titles saved: ../data/raw/ukb_data_duplicated_titles.txt\n",
      "Note: Not dropping (unique DOIs which should be retreivable): curious, though...\n",
      "Number of duplicated pub_id (inc. NaN):  0\n",
      "Number of NaNs in DOI column:  19\n",
      "These NaNs are saved out to:, ../data/raw/ukb_data_doi_nan.txt\n",
      "Note: At least some of these NaNs _should_ have DOIs, though...\n",
      "Number of records with no doi or pmid:  0\n",
      "Number of records which we are left with:  8532\n"
     ]
    }
   ],
   "source": [
    "df = get_ukb_showcase_data(timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e25cca7-eec8-40cc-880d-7f297ed2f190",
   "metadata": {},
   "source": [
    "Some things to note here. That there are duplicate DOIs is problematic (ignoring NaNs initially). This is because this is going to be our primary key to query on in the DSL (below). So, we end up dropping 21 of the non-unique DOIs (20 of the 41 duplicates remain). One entry in the UKBB showcase data has 3 entries with the same DOI. When we have no DOI, we can -- here -- rely on the records which have a PubMed ID (because at least initially, there are no NaNs for DOI which dont have a corresponding PMID). Some of the DOIs which have NaNs do actually have DOIs -- they can be found online, and some are actually returned from the DSL when when we query the relevant entries for PMID. Warrants consideration\\investigation upstream.\n",
    "\n",
    "There are 59 rows of duplicated titles. This isnt urgently problematic, because we are going to look up against DOIs and PMIDs below. But, it's indicative of something bad potentially happening upstream and could be investigated further. The duplicate titles and dois -- as well as NaN DOIs -- get saved out to `/data/raw/`.\n",
    "\n",
    "This leaves us with the number of rows printed at the bottom of the `get_ukb_showcase_data()` call, which is simply the number at the top minus the number of duplicated DOIs which get drops.\n",
    "\n",
    "Next, lets login to the DIMCLI to query the DSL. API key is stored privately and loaded in via `./keys/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "866f586a-4160-4a14-bdae-66717ae43edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mDimcli - Dimensions API Client (v1.4)\u001b[0m\n",
      "\u001b[2mConnected to: <https://app.dimensions.ai/api/dsl/v2> - DSL v2.10\u001b[0m\n",
      "\u001b[2mMethod: manual login\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<dimcli.Dsl #138623054175680. API endpoint: https://app.dimensions.ai/api/dsl/v2>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "login_dimcli()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e147f0-52d6-4991-b337-f47f2137d61e",
   "metadata": {},
   "source": [
    "OK, looks like this worked just fine. Now, we're going to get all Dimensions data related to these UKBB showcase rows. Our strategy -- as above -- is to first attempt to get all the DOIs (as the 'primary' search key), but where they are NaN, use PMID instead. We attempt to access ~200 rows at a time, to avoid HTTPErrors; if we hit a HTTPError, we keep retrying it (this seems to be because the query return is taking slightly too long). A log of the scrape gets printed to `./logging/` as a timestamped (to the second) file.\n",
    "\n",
    "A quick note: as we're timestamping our _data_ by the month, this checks to see whether we already have data for this month or not (we assume that the upstream file doesnt change more than once per month, and the first time we run it each month is the time at which data gets cached to that month). If we've already scraped the DSL for this month's data, it quickly just iterates to check that we have everything we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45c3d3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing doi chunks: 100%|████████████████| 43/43 [00:00<00:00, 176992.22it/s]\n",
      "Processing pmid chunks: 100%|███████████████████| 1/1 [00:00<00:00, 7182.03it/s]\n"
     ]
    }
   ],
   "source": [
    "get_raw_data(200, ['doi', 'pmid'], df, timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69501a7-7a1b-4cb9-b478-a96c681dc275",
   "metadata": {},
   "source": [
    "Next, we'll wrangle our DOI and PMID returns, merge them, and then evaluate our scrape of the DSL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24d48561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We start off expecting to get back this number of rows:  8532\n",
      "We expect to get this many doi:  8513\n",
      "We expect to get this many pmid:  19\n",
      "We actually get back this many dois:  8508\n",
      "We actually get back this unique dois:  8504\n",
      "Save out the 8/2 duplicates to  ../data/dimensions/api/raw/eval/202503/doi_scrape_duplicates.csv\n",
      "If a DOI is duplicated, keep the one which has reference_ids (or the first one seen).\n",
      "We now have this many dois:  8504\n",
      "The 9 DOIs not returned are saved at ../data/dimensions/api/raw/eval/202503/doi_not_in_dim.tsv\n",
      "Note: some of these are clearly non-indexed preprints.\n",
      "Note: Some of are on dimensions.ai app?'\n",
      "Note: Some of arent on dimensions.ai app, but look like they should be? e.g. 10.1038/s41588-018-0147-3\n",
      "We get this many from the pmid search:  19\n",
      "Cool, looks like we got all the pmids without issue\n",
      "We got this many rows of data from Dimensions:  8523\n",
      "Note: different to len(df) from i.) drop duplicates in wrangle_raw(), ii.) non-returns (sum to diff)\n"
     ]
    }
   ],
   "source": [
    "doi = merger(f'../data/dimensions/api/raw/doi/{timestamp}')\n",
    "pmid = merger(f'../data/dimensions/api/raw/pmid/{timestamp}')\n",
    "doi, pmid, df_dim = evaluate_raw_scrape(df, timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deecd402-5f48-4a8f-9d1e-ab3131a89d82",
   "metadata": {},
   "source": [
    "Some things to say about this. A very small number of input DOIs created >1 returns from the DSL -- this, probably, shouldn't happen, but is likely *their* fault. We keep ones which have reference IDs, or if neither have, just pick the first one relatively at random. This probably doesn't matter all that much, as we're still getting things like categories.\n",
    "\n",
    "What *is* a little more consequential, is the fact that some DOIs are not actually returned at all. Nothing we can really do about that, but some things to say. A couple of these are big papers which should be getting returned, but arent even on their webapp (e.g. an EA GWAS is missing). One or two look like they *are* on the web app, but arent getting returned from the API. Most are preprints, and it's likely fine that these aren't found at all (Dimensions hasn't scraped SSRN recently for example?). So, we lose a *very* small number of rows from the showcase data because of duplicate DOIs, and a *very* small number here of DOIs which dont get returned at all.\n",
    "\n",
    "Next, lets longways explode our `df_dim` dataframe to create source:target key-value pairs to scrape for references to each of these UKB Showcase papers. Note: we now move from using DOI to the Dimensions `id' file, as that's a far better UID within the DSL universe (ans is what is returned in the `reference_ids` field)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3b7f2d2-9e5c-448c-afe2-f504216bb261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows where reference_ids is an empty list: 74\n",
      "0 duplicates in the id-reference_id pair.\n",
      "Drop the NaN exploded reference ids (empty lists)\n",
      "Number of rows with NaN in either source_id or target_id: 0\n",
      "We have 408821 source:target id pairs,but only 192930 refs to get\n"
     ]
    }
   ],
   "source": [
    "df_dim, df_exploded = make_long_refs(df_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c15ed52-fe24-4967-8f68-473ac5cf421e",
   "metadata": {},
   "source": [
    "A surprisingly small number of papers have no references parsed (I expected a much higher % than this). Then take this long list of UIDs from the references, and push that back through the API. Again; if we already have this data, don't bother to get it again. Then, merge it using the same function as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "848c3745-7415-4275-b645-54d53a4e0ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing id chunks: 100%|███████████████| 965/965 [00:00<00:00, 444780.59it/s]\n"
     ]
    }
   ],
   "source": [
    "get_raw_data(200, ['id'], df_exploded, timestamp)\n",
    "refs = merger(f'../data/dimensions/api/raw/id/{timestamp}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc138fee-a621-4d3e-91bd-79fddd620ee7",
   "metadata": {},
   "source": [
    "Lets next evaluate again our scrape -- did we get all the data we expected to on the _references_?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7475e5f-8d92-46b5-a5c5-062d49b95d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the unique reference file is: 192909\n",
      "Length of the linked reference file is: 408798\n",
      "Length of rows of UKB data that have at least one reference:  8449\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print('Length of the unique reference file is:' , len(refs))\n",
    "df_exploded = pd.merge(df_exploded, refs, how='left',\n",
    "                       left_on='target_id', right_on='id')\n",
    "df_exploded = df_exploded[df_exploded['id'].notnull()].drop('id', axis=1)\n",
    "print('Length of the linked reference file is:' , len(df_exploded))\n",
    "print('Length of rows of UKB data that have at least one reference: ',\n",
    "      len(df_exploded['source_id'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54d26b3-502b-4809-85f8-4ce01afeb424",
   "metadata": {},
   "source": [
    "This is curious: it looks like a *tiny* number of references are not returned (99.989% are as of 202503!). I wonder why a truly tiny number of references aren't returned when they've been internally linked, and an explicit ID has been generated and has gone into the references of another paper? Likely their problem, and not of huge consequence for us, but good to document it all the same.\n",
    "It looks like this doesnt leave any unique UKB papers with no references, because the number of unique source_ids which are returned from this is the same as the number of unique ids which come out of the original scrape minus those with no references. Great accounting!\n",
    "\n",
    "Finally, save all these five (doi, pmid, df_dim, refs, df_exploded) wrangled files out to a dedicated subdirectory for downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5de846ed-cca3-4287-8dd1-3b16d4c60da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_combined(f'../data/dimensions/api/raw/combined/{timestamp}',\n",
    "              doi, pmid, df_dim, refs, df_exploded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
