{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-academic collaborations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "Load the Dimensions export used for institution parsing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# If you are missing dependencies, run:\n",
    "# %pip install pandas openpyxl openai\n",
    "\n",
    "DATA_REL = Path(\"data/dimensions/api/raw/combined/202511/df_dimensions.xlsx\")\n",
    "\n",
    "\n",
    "def find_data_path():\n",
    "    for base in (Path.cwd(), Path.cwd().parent):\n",
    "        candidate = base / DATA_REL\n",
    "        if candidate.exists():\n",
    "            return candidate\n",
    "    raise FileNotFoundError(f\"Could not find {DATA_REL}\")\n",
    "\n",
    "\n",
    "DATA_PATH = find_data_path()\n",
    "df = pd.read_excel(DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>acknowledgements</th>\n",
       "      <th>altmetric</th>\n",
       "      <th>altmetric_id</th>\n",
       "      <th>authors</th>\n",
       "      <th>authors_count</th>\n",
       "      <th>...</th>\n",
       "      <th>category_hra</th>\n",
       "      <th>funding_section</th>\n",
       "      <th>editors</th>\n",
       "      <th>subtitles</th>\n",
       "      <th>clinical_trial_ids</th>\n",
       "      <th>resulting_publication_doi</th>\n",
       "      <th>book_doi</th>\n",
       "      <th>book_title</th>\n",
       "      <th>isbn</th>\n",
       "      <th>proceedings_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>pub.1142697354</td>\n",
       "      <td>Adiposity by Differing Measures and the Risk o...</td>\n",
       "      <td>Purpose: To examine the association between ad...</td>\n",
       "      <td>The authors thank the participants of the UK B...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>117236054.0</td>\n",
       "      <td>[{'affiliations': [{'city': 'Guangzhou', 'city...</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>180</td>\n",
       "      <td>pub.1107515670</td>\n",
       "      <td>Independent and combined associations of mater...</td>\n",
       "      <td>Background: Limited evidence suggests that exp...</td>\n",
       "      <td>This research has been conducted using the UK ...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>50122557.0</td>\n",
       "      <td>[{'affiliations': [{'city': 'Bristol', 'city_i...</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This work was supported by the UK Medical Rese...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>172</td>\n",
       "      <td>pub.1111460082</td>\n",
       "      <td>Common conditions associated with hereditary h...</td>\n",
       "      <td>OBJECTIVE: To compare prevalent and incident m...</td>\n",
       "      <td>This research was conducted using the UK Bioba...</td>\n",
       "      <td>897.0</td>\n",
       "      <td>54023435.0</td>\n",
       "      <td>[{'affiliations': [{'city': 'Exeter', 'city_id...</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'id': '3903', 'name': 'Population &amp; Society'}]</td>\n",
       "      <td>Funding: This study was funded by an award to ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>173</td>\n",
       "      <td>pub.1111458679</td>\n",
       "      <td>Hereditary Hemochromatosis Associations with F...</td>\n",
       "      <td>BACKGROUND: Iron is essential for life but con...</td>\n",
       "      <td>This research has been conducted using the UK ...</td>\n",
       "      <td>54.0</td>\n",
       "      <td>54027903.0</td>\n",
       "      <td>[{'affiliations': [{'city': 'Exeter', 'city_id...</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'id': '3901', 'name': 'Clinical'}]</td>\n",
       "      <td>This work was supported by an award to D.M. by...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>174</td>\n",
       "      <td>pub.1111441688</td>\n",
       "      <td>Genetic Assessment of Potential Long-Term On-T...</td>\n",
       "      <td>BACKGROUND: Although short-term trials have su...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.0</td>\n",
       "      <td>54234074.0</td>\n",
       "      <td>[{'affiliations': [{'city': 'Leicester', 'city...</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'id': '3900', 'name': 'Biomedical'}]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0.1  Unnamed: 0              id  \\\n",
       "0             0           0  pub.1142697354   \n",
       "1             1         180  pub.1107515670   \n",
       "2             2         172  pub.1111460082   \n",
       "3             3         173  pub.1111458679   \n",
       "4             4         174  pub.1111441688   \n",
       "\n",
       "                                               title  \\\n",
       "0  Adiposity by Differing Measures and the Risk o...   \n",
       "1  Independent and combined associations of mater...   \n",
       "2  Common conditions associated with hereditary h...   \n",
       "3  Hereditary Hemochromatosis Associations with F...   \n",
       "4  Genetic Assessment of Potential Long-Term On-T...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Purpose: To examine the association between ad...   \n",
       "1  Background: Limited evidence suggests that exp...   \n",
       "2  OBJECTIVE: To compare prevalent and incident m...   \n",
       "3  BACKGROUND: Iron is essential for life but con...   \n",
       "4  BACKGROUND: Although short-term trials have su...   \n",
       "\n",
       "                                    acknowledgements  altmetric  altmetric_id  \\\n",
       "0  The authors thank the participants of the UK B...        1.0   117236054.0   \n",
       "1  This research has been conducted using the UK ...        8.0    50122557.0   \n",
       "2  This research was conducted using the UK Bioba...      897.0    54023435.0   \n",
       "3  This research has been conducted using the UK ...       54.0    54027903.0   \n",
       "4                                                NaN       21.0    54234074.0   \n",
       "\n",
       "                                             authors  authors_count  ...  \\\n",
       "0  [{'affiliations': [{'city': 'Guangzhou', 'city...             12  ...   \n",
       "1  [{'affiliations': [{'city': 'Bristol', 'city_i...              5  ...   \n",
       "2  [{'affiliations': [{'city': 'Exeter', 'city_id...              9  ...   \n",
       "3  [{'affiliations': [{'city': 'Exeter', 'city_id...              7  ...   \n",
       "4  [{'affiliations': [{'city': 'Leicester', 'city...              7  ...   \n",
       "\n",
       "                                       category_hra  \\\n",
       "0                                               NaN   \n",
       "1                                               NaN   \n",
       "2  [{'id': '3903', 'name': 'Population & Society'}]   \n",
       "3              [{'id': '3901', 'name': 'Clinical'}]   \n",
       "4            [{'id': '3900', 'name': 'Biomedical'}]   \n",
       "\n",
       "                                     funding_section editors subtitles  \\\n",
       "0                                                NaN     NaN       NaN   \n",
       "1  This work was supported by the UK Medical Rese...     NaN       NaN   \n",
       "2  Funding: This study was funded by an award to ...     NaN       NaN   \n",
       "3  This work was supported by an award to D.M. by...     NaN       NaN   \n",
       "4                                                NaN     NaN       NaN   \n",
       "\n",
       "  clinical_trial_ids resulting_publication_doi book_doi book_title isbn  \\\n",
       "0                NaN                       NaN      NaN        NaN  NaN   \n",
       "1                NaN                       NaN      NaN        NaN  NaN   \n",
       "2                NaN                       NaN      NaN        NaN  NaN   \n",
       "3                NaN                       NaN      NaN        NaN  NaN   \n",
       "4                NaN                       NaN      NaN        NaN  NaN   \n",
       "\n",
       "  proceedings_title  \n",
       "0               NaN  \n",
       "1               NaN  \n",
       "2               NaN  \n",
       "3               NaN  \n",
       "4               NaN  \n",
       "\n",
       "[5 rows x 75 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: check available OpenAI models\n",
    "\n",
    "Useful if you want to verify available model IDs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gpt-4o-mini-tts-2025-12-15',\n",
       " 'gpt-realtime-mini-2025-12-15',\n",
       " 'gpt-audio-mini-2025-12-15',\n",
       " 'gpt-4o-mini-2024-07-18',\n",
       " 'gpt-4o-mini',\n",
       " 'gpt-4o-mini-realtime-preview-2024-12-17',\n",
       " 'gpt-4o-mini-audio-preview-2024-12-17',\n",
       " 'gpt-4o-mini-realtime-preview',\n",
       " 'gpt-4o-mini-audio-preview',\n",
       " 'o3-mini',\n",
       " 'o3-mini-2025-01-31',\n",
       " 'gpt-4o-mini-search-preview-2025-03-11',\n",
       " 'gpt-4o-mini-search-preview',\n",
       " 'gpt-4o-mini-transcribe',\n",
       " 'gpt-4o-mini-tts',\n",
       " 'o4-mini-2025-04-16',\n",
       " 'o4-mini',\n",
       " 'gpt-4.1-mini-2025-04-14',\n",
       " 'gpt-4.1-mini',\n",
       " 'gpt-4.1-nano-2025-04-14',\n",
       " 'gpt-4.1-nano',\n",
       " 'codex-mini-latest',\n",
       " 'o4-mini-deep-research',\n",
       " 'o4-mini-deep-research-2025-06-26',\n",
       " 'gpt-5-mini-2025-08-07',\n",
       " 'gpt-5-mini',\n",
       " 'gpt-5-nano-2025-08-07',\n",
       " 'gpt-5-nano',\n",
       " 'gpt-image-1-mini',\n",
       " 'gpt-audio-mini',\n",
       " 'gpt-audio-mini-2025-10-06',\n",
       " 'gpt-realtime-mini',\n",
       " 'gpt-realtime-mini-2025-10-06',\n",
       " 'gpt-5.1-codex-mini',\n",
       " 'gpt-4o-mini-transcribe-2025-12-15',\n",
       " 'gpt-4o-mini-transcribe-2025-03-20',\n",
       " 'gpt-4o-mini-tts-2025-03-20']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from pathlib import Path\n",
    "\n",
    "api_key = Path(\"../keys/openai.txt\").read_text().strip()\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "models = [m.id for m in client.models.list().data]\n",
    "[m for m in models if \"mini\" in m or \"nano\" in m]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify institution-related columns\n",
    "\n",
    "Pick author/institution columns for parsing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['research_org_cities',\n",
       " 'research_org_countries',\n",
       " 'research_org_country_names',\n",
       " 'research_org_names',\n",
       " 'research_org_state_codes',\n",
       " 'research_org_state_names',\n",
       " 'research_org_types',\n",
       " 'research_orgs']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify author institution-related columns. Adjust this heuristic if needed.\n",
    "name_keys = (\"institution\", \"affiliation\", \"affiliations\", \"organization\", \"organisation\", \"org\")\n",
    "author_cols = [c for c in df.columns if \"author\" in c.lower()]\n",
    "institution_cols = [c for c in df.columns if any(k in c.lower() for k in name_keys)]\n",
    "author_institution_cols = [c for c in institution_cols if \"author\" in c.lower()] or institution_cols\n",
    "\n",
    "author_institution_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>research_org_cities</th>\n",
       "      <th>research_org_countries</th>\n",
       "      <th>research_org_country_names</th>\n",
       "      <th>research_org_names</th>\n",
       "      <th>research_org_state_codes</th>\n",
       "      <th>research_org_state_names</th>\n",
       "      <th>research_org_types</th>\n",
       "      <th>research_orgs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'id': '6952201', 'name': 'East Melbourne'}, ...</td>\n",
       "      <td>[{'id': 'CN', 'name': 'China'}, {'id': 'AU', '...</td>\n",
       "      <td>['China', 'Australia']</td>\n",
       "      <td>['Sun Yat-sen University', 'Centre for Eye Res...</td>\n",
       "      <td>[{'id': 'AU-VIC', 'name': 'Victoria'}]</td>\n",
       "      <td>['Victoria']</td>\n",
       "      <td>['Education', 'Facility', 'Healthcare']</td>\n",
       "      <td>[{'acronym': 'CERA', 'city_name': 'East Melbou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'id': '2654675', 'name': 'Bristol'}, {'id': ...</td>\n",
       "      <td>[{'id': 'GB', 'name': 'United Kingdom'}, {'id'...</td>\n",
       "      <td>['United Kingdom', 'Norway']</td>\n",
       "      <td>['Norwegian Institute of Public Health', 'Univ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Government', 'Education', 'Facility']</td>\n",
       "      <td>[{'city_name': 'Bristol', 'country_code': 'GB'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'id': '2649808', 'name': 'Exeter'}, {'id': '...</td>\n",
       "      <td>[{'id': 'GB', 'name': 'United Kingdom'}, {'id'...</td>\n",
       "      <td>['United States', 'United Kingdom']</td>\n",
       "      <td>['University of Exeter', 'University of Connec...</td>\n",
       "      <td>[{'id': 'US-MD', 'name': 'Maryland'}, {'id': '...</td>\n",
       "      <td>['Connecticut', 'Maryland']</td>\n",
       "      <td>['Government', 'Education', 'Healthcare']</td>\n",
       "      <td>[{'acronym': 'UCHC', 'city_name': 'Farmington'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'id': '2649808', 'name': 'Exeter'}, {'id': '...</td>\n",
       "      <td>[{'id': 'GB', 'name': 'United Kingdom'}, {'id'...</td>\n",
       "      <td>['United States', 'United Kingdom']</td>\n",
       "      <td>['University of Connecticut', 'University of E...</td>\n",
       "      <td>[{'id': 'US-MD', 'name': 'Maryland'}, {'id': '...</td>\n",
       "      <td>['Connecticut', 'Maryland']</td>\n",
       "      <td>['Government', 'Education', 'Healthcare']</td>\n",
       "      <td>[{'acronym': 'UCHC', 'city_name': 'Farmington'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{'id': '2867714', 'name': 'Munich'}, {'id': '...</td>\n",
       "      <td>[{'id': 'GB', 'name': 'United Kingdom'}, {'id'...</td>\n",
       "      <td>['United Kingdom', 'Germany']</td>\n",
       "      <td>['German Centre for Cardiovascular Research', ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Facility', 'Healthcare', 'Education']</td>\n",
       "      <td>[{'city_name': 'Leicester', 'country_code': 'G...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 research_org_cities  \\\n",
       "0  [{'id': '6952201', 'name': 'East Melbourne'}, ...   \n",
       "1  [{'id': '2654675', 'name': 'Bristol'}, {'id': ...   \n",
       "2  [{'id': '2649808', 'name': 'Exeter'}, {'id': '...   \n",
       "3  [{'id': '2649808', 'name': 'Exeter'}, {'id': '...   \n",
       "4  [{'id': '2867714', 'name': 'Munich'}, {'id': '...   \n",
       "\n",
       "                              research_org_countries  \\\n",
       "0  [{'id': 'CN', 'name': 'China'}, {'id': 'AU', '...   \n",
       "1  [{'id': 'GB', 'name': 'United Kingdom'}, {'id'...   \n",
       "2  [{'id': 'GB', 'name': 'United Kingdom'}, {'id'...   \n",
       "3  [{'id': 'GB', 'name': 'United Kingdom'}, {'id'...   \n",
       "4  [{'id': 'GB', 'name': 'United Kingdom'}, {'id'...   \n",
       "\n",
       "            research_org_country_names  \\\n",
       "0               ['China', 'Australia']   \n",
       "1         ['United Kingdom', 'Norway']   \n",
       "2  ['United States', 'United Kingdom']   \n",
       "3  ['United States', 'United Kingdom']   \n",
       "4        ['United Kingdom', 'Germany']   \n",
       "\n",
       "                                  research_org_names  \\\n",
       "0  ['Sun Yat-sen University', 'Centre for Eye Res...   \n",
       "1  ['Norwegian Institute of Public Health', 'Univ...   \n",
       "2  ['University of Exeter', 'University of Connec...   \n",
       "3  ['University of Connecticut', 'University of E...   \n",
       "4  ['German Centre for Cardiovascular Research', ...   \n",
       "\n",
       "                            research_org_state_codes  \\\n",
       "0             [{'id': 'AU-VIC', 'name': 'Victoria'}]   \n",
       "1                                                NaN   \n",
       "2  [{'id': 'US-MD', 'name': 'Maryland'}, {'id': '...   \n",
       "3  [{'id': 'US-MD', 'name': 'Maryland'}, {'id': '...   \n",
       "4                                                NaN   \n",
       "\n",
       "      research_org_state_names                         research_org_types  \\\n",
       "0                 ['Victoria']    ['Education', 'Facility', 'Healthcare']   \n",
       "1                          NaN    ['Government', 'Education', 'Facility']   \n",
       "2  ['Connecticut', 'Maryland']  ['Government', 'Education', 'Healthcare']   \n",
       "3  ['Connecticut', 'Maryland']  ['Government', 'Education', 'Healthcare']   \n",
       "4                          NaN    ['Facility', 'Healthcare', 'Education']   \n",
       "\n",
       "                                       research_orgs  \n",
       "0  [{'acronym': 'CERA', 'city_name': 'East Melbou...  \n",
       "1  [{'city_name': 'Bristol', 'country_code': 'GB'...  \n",
       "2  [{'acronym': 'UCHC', 'city_name': 'Farmington'...  \n",
       "3  [{'acronym': 'UCHC', 'city_name': 'Farmington'...  \n",
       "4  [{'city_name': 'Leicester', 'country_code': 'G...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[author_institution_cols].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse institution strings\n",
    "\n",
    "Normalize, split, and deduplicate institution-like strings from the author fields.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "import re\n",
    "\n",
    "INSTITUTION_KEYS = {\n",
    "    \"name\",\n",
    "    \"institution\",\n",
    "    \"organization\",\n",
    "    \"organisation\",\n",
    "    \"org\",\n",
    "    \"affiliation\",\n",
    "    \"affiliations\",\n",
    "    \"raw_affiliation\",\n",
    "}\n",
    "\n",
    "KEYWORDS = (\n",
    "    \"university\",\n",
    "    \"college\",\n",
    "    \"institute\",\n",
    "    \"institut\",\n",
    "    \"school\",\n",
    "    \"hospital\",\n",
    "    \"clinic\",\n",
    "    \"centre\",\n",
    "    \"center\",\n",
    "    \"foundation\",\n",
    "    \"trust\",\n",
    "    \"ministry\",\n",
    "    \"government\",\n",
    "    \"council\",\n",
    "    \"company\",\n",
    "    \"inc\",\n",
    "    \"ltd\",\n",
    "    \"llc\",\n",
    "    \"gmbh\",\n",
    "    \"corp\",\n",
    "    \"plc\",\n",
    "    \"sa\",\n",
    "    \"srl\",\n",
    "    \"bv\",\n",
    "    \"ag\",\n",
    "    \"laboratory\",\n",
    "    \"lab\",\n",
    ")\n",
    "\n",
    "MAX_INSTITUTIONS_PER_ROW = 200\n",
    "MAX_CHARS_PER_INSTITUTION = 150\n",
    "\n",
    "_SPLIT_RE = re.compile(r\"[;|]+\")\n",
    "\n",
    "\n",
    "def _normalize_text(s):\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    s = re.sub(r\"\\S+@\\S+\", \"\", s)\n",
    "    s = re.sub(r\"https?://\\S+\", \"\", s)\n",
    "    return s.strip(\" ,;|\")\n",
    "\n",
    "\n",
    "def _contains_keyword(s):\n",
    "    s = s.lower()\n",
    "    return any(k in s for k in KEYWORDS)\n",
    "\n",
    "\n",
    "def _try_parse(s):\n",
    "    s = s.strip()\n",
    "    if not s or s[0] not in \"[{\":\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        try:\n",
    "            return ast.literal_eval(s)\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "\n",
    "def _extract_strings(obj):\n",
    "    strings = []\n",
    "    if obj is None:\n",
    "        return strings\n",
    "    if isinstance(obj, (list, tuple, set)):\n",
    "        for item in obj:\n",
    "            strings.extend(_extract_strings(item))\n",
    "        return strings\n",
    "    if isinstance(obj, dict):\n",
    "        for key, value in obj.items():\n",
    "            if key in INSTITUTION_KEYS:\n",
    "                strings.extend(_extract_strings(value))\n",
    "        return strings\n",
    "    strings.append(str(obj))\n",
    "    return strings\n",
    "\n",
    "\n",
    "def _select_institution_segments(s):\n",
    "    cleaned = _normalize_text(s)\n",
    "    if not cleaned:\n",
    "        return []\n",
    "    parts = [p.strip() for p in _SPLIT_RE.split(cleaned) if p.strip()]\n",
    "    selected = []\n",
    "    for part in parts:\n",
    "        comma_parts = [p.strip() for p in part.split(\",\") if p.strip()]\n",
    "        if comma_parts:\n",
    "            keyword_part = next((p for p in comma_parts if _contains_keyword(p)), None)\n",
    "            part = keyword_part or comma_parts[0]\n",
    "        part = _normalize_text(part)\n",
    "        if part:\n",
    "            if len(part) > MAX_CHARS_PER_INSTITUTION:\n",
    "                part = part[:MAX_CHARS_PER_INSTITUTION].rstrip()\n",
    "            selected.append(part)\n",
    "    if not any(_contains_keyword(p) for p in selected):\n",
    "        selected = selected[:2]\n",
    "    return selected\n",
    "\n",
    "\n",
    "def _extract_institutions_from_value(value):\n",
    "    if value is None or (isinstance(value, float) and str(value) == \"nan\"):\n",
    "        return []\n",
    "    if isinstance(value, str):\n",
    "        parsed = _try_parse(value)\n",
    "        if parsed is not None:\n",
    "            strings = _extract_strings(parsed)\n",
    "        else:\n",
    "            strings = [value]\n",
    "    else:\n",
    "        strings = _extract_strings(value)\n",
    "\n",
    "    candidates = []\n",
    "    for s in strings:\n",
    "        candidates.extend(_select_institution_segments(s))\n",
    "    return candidates\n",
    "\n",
    "\n",
    "def _dedupe_keep_order(items):\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for item in items:\n",
    "        key = item.lower()\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        out.append(item)\n",
    "    return out\n",
    "\n",
    "\n",
    "def build_institution_list(row):\n",
    "    candidates = []\n",
    "    for value in row:\n",
    "        candidates.extend(_extract_institutions_from_value(value))\n",
    "    candidates = _dedupe_keep_order(candidates)\n",
    "    return candidates[:MAX_INSTITUTIONS_PER_ROW]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build institution lists per row\n",
    "\n",
    "Create a list field and a compact text preview for the LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_institutions_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>East Melbourne | Guangzhou | Melbourne | China...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bristol | Oslo | United Kingdom | Norway | Nor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Exeter | Farmington | Baltimore | United Kingd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Exeter | Farmington | Storrs | Baltimore | Uni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Munich | Leicester | Berlin | United Kingdom |...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            author_institutions_text\n",
       "0  East Melbourne | Guangzhou | Melbourne | China...\n",
       "1  Bristol | Oslo | United Kingdom | Norway | Nor...\n",
       "2  Exeter | Farmington | Baltimore | United Kingd...\n",
       "3  Exeter | Farmington | Storrs | Baltimore | Uni...\n",
       "4  Munich | Leicester | Berlin | United Kingdom |..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list per row for model input and a compact text preview.\n",
    "author_institutions = df[author_institution_cols].copy().fillna(\"\")\n",
    "\n",
    "df[\"author_institutions_list\"] = author_institutions.apply(\n",
    "    build_institution_list, axis=1\n",
    ")\n",
    "\n",
    "df[\"author_institutions_text\"] = df[\"author_institutions_list\"].map(\n",
    "    lambda xs: \" | \".join(xs)\n",
    ")\n",
    "\n",
    "df[[\"author_institutions_text\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM client configuration\n",
    "\n",
    "Set up OpenAI client and shared request settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import asyncio\n",
    "import ast\n",
    "import re\n",
    "\n",
    "try:\n",
    "    from openai import AsyncOpenAI\n",
    "    HAS_ASYNC = True\n",
    "except Exception:\n",
    "    AsyncOpenAI = None\n",
    "    HAS_ASYNC = False\n",
    "\n",
    "KEY_REL = Path(\"keys/openai.txt\")\n",
    "\n",
    "\n",
    "def find_key_path():\n",
    "    for base in (Path.cwd(), Path.cwd().parent):\n",
    "        candidate = base / KEY_REL\n",
    "        if candidate.exists():\n",
    "            return candidate\n",
    "    raise FileNotFoundError(f\"Could not find {KEY_REL}\")\n",
    "\n",
    "\n",
    "api_key = find_key_path().read_text().strip()\n",
    "client = OpenAI(api_key=api_key)\n",
    "async_client = AsyncOpenAI(api_key=api_key) if HAS_ASYNC else None\n",
    "\n",
    "MODEL = \"gpt-5-nano\"\n",
    "SERVICE_TIER = \"priority\"\n",
    "SERVICE_TIER_MAP = {\"standard\": \"default\"}\n",
    "\n",
    "REQUEST_SLEEP = 0.0  # set to a positive number if you need to slow requests\n",
    "REQUEST_TIMEOUT = 120\n",
    "\n",
    "\n",
    "# Determinism controls (best-effort; full determinism is not guaranteed)\n",
    "# Note: gpt-5-mini does not accept temperature, so leave it as None.\n",
    "TEMPERATURE = None\n",
    "TOP_P = 1\n",
    "FREQUENCY_PENALTY = 0\n",
    "PRESENCE_PENALTY = 0\n",
    "SEED = 12345  # set to None to disable\n",
    "RESPONSE_FORMAT = {\"type\": \"json_object\"}\n",
    "MAX_OUTPUT_TOKENS = 2000  # increase if batches are large\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You classify institution strings as academic organizations, non-academic organizations, \"\n",
    "    \"and UK-based companies. Only label entries that are clearly organizations. \"\n",
    "    \"Ignore locations (cities, countries, states), departments, faculties, and generic terms \"\n",
    "    \"like 'Education', 'Facility', or 'Healthcare'. \"\n",
    "    \"Academic: universities, colleges, degree-granting schools, and explicitly \"\n",
    "    \"university-affiliated hospitals. \"\n",
    "    \"Non-academic: companies, industry, private sector, NGOs, charities, foundations, \"\n",
    "    \"government agencies, and non-university hospitals or clinics. \"\n",
    "    \"UK company: a private-sector company clearly based in the United Kingdom \"\n",
    "    \"(explicit UK/United Kingdom/England/Scotland/Wales/Northern Ireland signal or well-known UK company). \"\n",
    "    \"If you are unsure about UK or organizational status, do NOT label it.\"\n",
    ")\n",
    "\n",
    "STRICT_JSON_INSTRUCTIONS = (\n",
    "    \"Return ONLY valid JSON. Use double quotes for all keys and strings. No trailing commas.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM request and parsing helpers\n",
    "\n",
    "Build prompts, call the API, and parse JSON safely.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b0dbcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_param(msg, marker):\n",
    "    if marker not in msg:\n",
    "        return None\n",
    "    after = msg.split(marker, 1)[1].strip()\n",
    "    if not after:\n",
    "        return None\n",
    "    if after[0] in (\"'\", '\"'):\n",
    "        quote = after[0]\n",
    "        end = after.find(quote, 1)\n",
    "        if end != -1:\n",
    "            return after[1:end]\n",
    "    value = after.split()[0].strip()\n",
    "    if value.startswith((\"'\", '\"')):\n",
    "        value = value[1:]\n",
    "    if value.endswith((\"'\", '\"')):\n",
    "        value = value[:-1]\n",
    "    return value\n",
    "\n",
    "\n",
    "def _remove_unsupported_param(msg, kwargs):\n",
    "    for marker in (\"Unsupported parameter:\", \"unexpected keyword argument\"):\n",
    "        key = _extract_param(msg, marker)\n",
    "        if key and key in kwargs:\n",
    "            kwargs.pop(key, None)\n",
    "            return True\n",
    "    if \"service_tier\" in msg and \"invalid\" in msg:\n",
    "        if \"service_tier\" in kwargs:\n",
    "            kwargs.pop(\"service_tier\", None)\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def _build_response_kwargs(messages):\n",
    "    kwargs = {\n",
    "        \"model\": MODEL,\n",
    "        \"input\": messages,\n",
    "    }\n",
    "    if RESPONSE_FORMAT is not None:\n",
    "        kwargs[\"response_format\"] = RESPONSE_FORMAT\n",
    "    if TEMPERATURE is not None:\n",
    "        kwargs[\"temperature\"] = TEMPERATURE\n",
    "    if TOP_P is not None:\n",
    "        kwargs[\"top_p\"] = TOP_P\n",
    "    if FREQUENCY_PENALTY is not None:\n",
    "        kwargs[\"frequency_penalty\"] = FREQUENCY_PENALTY\n",
    "    if PRESENCE_PENALTY is not None:\n",
    "        kwargs[\"presence_penalty\"] = PRESENCE_PENALTY\n",
    "    if SERVICE_TIER:\n",
    "        kwargs[\"service_tier\"] = SERVICE_TIER_MAP.get(SERVICE_TIER, SERVICE_TIER)\n",
    "    if SEED is not None:\n",
    "        kwargs[\"seed\"] = SEED\n",
    "    if MAX_OUTPUT_TOKENS is not None:\n",
    "        kwargs[\"max_output_tokens\"] = MAX_OUTPUT_TOKENS\n",
    "    return kwargs\n",
    "\n",
    "\n",
    "def _build_chat_kwargs(messages):\n",
    "    kwargs = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": messages,\n",
    "    }\n",
    "    if RESPONSE_FORMAT is not None:\n",
    "        kwargs[\"response_format\"] = RESPONSE_FORMAT\n",
    "    if TEMPERATURE is not None:\n",
    "        kwargs[\"temperature\"] = TEMPERATURE\n",
    "    if TOP_P is not None:\n",
    "        kwargs[\"top_p\"] = TOP_P\n",
    "    if FREQUENCY_PENALTY is not None:\n",
    "        kwargs[\"frequency_penalty\"] = FREQUENCY_PENALTY\n",
    "    if PRESENCE_PENALTY is not None:\n",
    "        kwargs[\"presence_penalty\"] = PRESENCE_PENALTY\n",
    "    if SEED is not None:\n",
    "        kwargs[\"seed\"] = SEED\n",
    "    if MAX_OUTPUT_TOKENS is not None:\n",
    "        kwargs[\"max_tokens\"] = MAX_OUTPUT_TOKENS\n",
    "    return kwargs\n",
    "\n",
    "\n",
    "def _call_with_param_retry(call_fn, kwargs):\n",
    "    for _ in range(5):\n",
    "        try:\n",
    "            return call_fn(**kwargs)\n",
    "        except Exception as exc:\n",
    "            msg = str(exc)\n",
    "            if _remove_unsupported_param(msg, kwargs):\n",
    "                continue\n",
    "            raise\n",
    "    raise RuntimeError(\"Repeated unsupported-parameter errors; check request params.\")\n",
    "\n",
    "\n",
    "def call_model(system_prompt, user_prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "    if hasattr(client, \"responses\"):\n",
    "        kwargs = _build_response_kwargs(messages)\n",
    "        response = _call_with_param_retry(client.responses.create, kwargs)\n",
    "        return response.output_text\n",
    "\n",
    "    # Fallback for older clients\n",
    "    kwargs = _build_chat_kwargs(messages)\n",
    "    response = _call_with_param_retry(client.chat.completions.create, kwargs)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "async def _acall_with_param_retry(call_fn, kwargs):\n",
    "    for _ in range(5):\n",
    "        try:\n",
    "            return await call_fn(**kwargs)\n",
    "        except Exception as exc:\n",
    "            msg = str(exc)\n",
    "            if _remove_unsupported_param(msg, kwargs):\n",
    "                continue\n",
    "            raise\n",
    "    raise RuntimeError(\"Repeated unsupported-parameter errors; check request params.\")\n",
    "\n",
    "\n",
    "async def call_model_async(system_prompt, user_prompt):\n",
    "    if not HAS_ASYNC:\n",
    "        return await asyncio.to_thread(call_model, system_prompt, user_prompt)\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "    if hasattr(async_client, \"responses\"):\n",
    "        kwargs = _build_response_kwargs(messages)\n",
    "        response = await _acall_with_param_retry(async_client.responses.create, kwargs)\n",
    "        return response.output_text\n",
    "\n",
    "    kwargs = _build_chat_kwargs(messages)\n",
    "    response = await _acall_with_param_retry(async_client.chat.completions.create, kwargs)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def _extract_json_fragment(text):\n",
    "    starts = [i for i in (text.find(\"{\"), text.find(\"[\")) if i != -1]\n",
    "    if not starts:\n",
    "        return None\n",
    "    start = min(starts)\n",
    "\n",
    "    stack = []\n",
    "    in_str = False\n",
    "    escape = False\n",
    "\n",
    "    for i in range(start, len(text)):\n",
    "        ch = text[i]\n",
    "        if in_str:\n",
    "            if escape:\n",
    "                escape = False\n",
    "            elif ch == \"\\\\\":\n",
    "                escape = True\n",
    "            elif ch == '\"':\n",
    "                in_str = False\n",
    "            continue\n",
    "\n",
    "        if ch == '\"':\n",
    "            in_str = True\n",
    "            continue\n",
    "        if ch in \"{[\":\n",
    "            stack.append(ch)\n",
    "            continue\n",
    "        if ch in \"]}\":\n",
    "            if not stack:\n",
    "                continue\n",
    "            open_ch = stack.pop()\n",
    "            if (open_ch == \"{\" and ch != \"}\") or (open_ch == \"[\" and ch != \"]\"):\n",
    "                return None\n",
    "            if not stack:\n",
    "                return text[start : i + 1]\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def _parse_json_object(raw):\n",
    "    text = raw.strip()\n",
    "    candidates = [text]\n",
    "    fragment = _extract_json_fragment(text)\n",
    "    if fragment and fragment not in candidates:\n",
    "        candidates.append(fragment)\n",
    "\n",
    "    last_exc = None\n",
    "    for candidate in candidates:\n",
    "        try:\n",
    "            return json.loads(candidate)\n",
    "        except json.JSONDecodeError as exc:\n",
    "            last_exc = exc\n",
    "\n",
    "    for candidate in candidates:\n",
    "        try:\n",
    "            return ast.literal_eval(candidate)\n",
    "        except Exception as exc:\n",
    "            last_exc = exc\n",
    "\n",
    "    raise last_exc\n",
    "\n",
    "\n",
    "def _build_batch_prompt(rows, strict=False):\n",
    "    prompt = (\n",
    "        \"Input is a JSON array of rows. Each row is a list of institution strings. \"\n",
    "        \"For each row, return an object with keys: non_academic_indices, academic_indices, \"\n",
    "        \"uk_company_indices (each a list of 0-based integer positions in that row). \"\n",
    "        \"Only include indices when you are confident the string is an organization of that type. \"\n",
    "        \"uk_company_indices must be a subset of non_academic_indices. \"\n",
    "        \"If there are none, use empty lists. \"\n",
    "        'Return exactly: {\"results\": [...]} with the same length and order as the input.'\n",
    "    )\n",
    "    prompt += \"\\n\"\n",
    "    if strict:\n",
    "        prompt += STRICT_JSON_INSTRUCTIONS + \"\\n\"\n",
    "    prompt += f\"Input:\\n{json.dumps(rows, ensure_ascii=True)}\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def _extract_results(data):\n",
    "    if isinstance(data, list):\n",
    "        return data\n",
    "    if isinstance(data, dict):\n",
    "        return data.get(\"results\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def classify_non_academic_batch(rows, strict=False):\n",
    "    if not rows:\n",
    "        return []\n",
    "\n",
    "    user_prompt = _build_batch_prompt(rows, strict=strict)\n",
    "    raw = call_model(SYSTEM_PROMPT, user_prompt)\n",
    "    data = _parse_json_object(raw)\n",
    "    results = _extract_results(data)\n",
    "    if results is None:\n",
    "        raise ValueError(\"Missing 'results' in model response.\")\n",
    "    if len(results) != len(rows):\n",
    "        raise ValueError(\"Batch result length mismatch.\")\n",
    "    return results\n",
    "\n",
    "\n",
    "async def classify_non_academic_batch_async(rows, strict=False):\n",
    "    if not rows:\n",
    "        return []\n",
    "\n",
    "    user_prompt = _build_batch_prompt(rows, strict=strict)\n",
    "    try:\n",
    "        raw = await asyncio.wait_for(\n",
    "            call_model_async(SYSTEM_PROMPT, user_prompt),\n",
    "            timeout=REQUEST_TIMEOUT,\n",
    "        )\n",
    "    except asyncio.TimeoutError as exc:\n",
    "        raise TimeoutError(f\"Request timed out after {REQUEST_TIMEOUT}s\") from exc\n",
    "    data = _parse_json_object(raw)\n",
    "    results = _extract_results(data)\n",
    "    if results is None:\n",
    "        raise ValueError(\"Missing 'results' in model response.\")\n",
    "    if len(results) != len(rows):\n",
    "        raise ValueError(\"Batch result length mismatch.\")\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch classification settings\n",
    "\n",
    "Configure batching, output paths, and prep the rows for classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run classification with caching and batching to reduce API calls.\n",
    "# If you are missing tqdm, run:\n",
    "# %pip install tqdm\n",
    "import warnings\n",
    "from tqdm import TqdmWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=TqdmWarning)\n",
    "from tqdm.auto import tqdm\n",
    "import asyncio\n",
    "\n",
    "MAX_ROWS = None  # set to an int to test on a subset\n",
    "BATCH_SIZE = 20\n",
    "MAX_CONCURRENCY = 16\n",
    "STRICT_RETRY = False\n",
    "MAX_FALLBACK_SPLITS = 1\n",
    "\n",
    "SAVE_PROGRESS_CSV = True\n",
    "SAVE_PROGRESS_EVERY_N = 5\n",
    "SAVE_EXCEL_EVERY_N = None\n",
    "SAVE_EXCEL_AT_END = True\n",
    "\n",
    "LOAD_CACHE = True\n",
    "APPEND_CACHE = True\n",
    "\n",
    "VERBOSE = False\n",
    "\n",
    "OUTPUT_REL = Path(\"output/non_academic_flagged.xlsx\")\n",
    "\n",
    "\n",
    "def resolve_output_path():\n",
    "    for base in (Path.cwd(), Path.cwd().parent):\n",
    "        candidate = base / OUTPUT_REL\n",
    "        if candidate.parent.exists():\n",
    "            return candidate\n",
    "    return Path.cwd() / \"non_academic_flagged.xlsx\"\n",
    "\n",
    "\n",
    "OUTPUT_PATH = resolve_output_path()\n",
    "PROGRESS_CSV_PATH = OUTPUT_PATH.with_suffix(\".progress.csv\")\n",
    "CACHE_PATH = OUTPUT_PATH.with_suffix(\".cache.jsonl\")\n",
    "\n",
    "BASE_RESULT = {\n",
    "    \"non_academic_indices\": [],\n",
    "    \"academic_indices\": [],\n",
    "    \"uk_company_indices\": [],\n",
    "}\n",
    "\n",
    "failed_rows = []\n",
    "\n",
    "df_work = df.copy()\n",
    "if MAX_ROWS is not None:\n",
    "    df_work = df_work.head(MAX_ROWS).copy()\n",
    "\n",
    "rows = df_work[\"author_institutions_list\"].tolist()\n",
    "row_keys = [tuple(r) for r in rows]\n",
    "unique_keys = list(dict.fromkeys(row_keys))\n",
    "\n",
    "\n",
    "def make_batches(seq, size):\n",
    "    for i in range(0, len(seq), size):\n",
    "        yield seq[i : i + size]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize model outputs\n",
    "\n",
    "Convert indices to institution lists and add UK company flags.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdd1cc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normalize_indices(value, max_len):\n",
    "    if not isinstance(value, list):\n",
    "        return []\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for idx in value:\n",
    "        try:\n",
    "            i = int(idx)\n",
    "        except Exception:\n",
    "            continue\n",
    "        if 0 <= i < max_len and i not in seen:\n",
    "            seen.add(i)\n",
    "            out.append(i)\n",
    "    return out\n",
    "\n",
    "\n",
    "def _indices_to_institutions(row_list, indices):\n",
    "    return [row_list[i] for i in indices]\n",
    "\n",
    "\n",
    "def normalize_result(row_list, result):\n",
    "    if not isinstance(result, dict):\n",
    "        result = {}\n",
    "    n = len(row_list)\n",
    "\n",
    "    non_idx = _normalize_indices(result.get(\"non_academic_indices\", []), n)\n",
    "    acad_idx = _normalize_indices(result.get(\"academic_indices\", []), n)\n",
    "    uk_idx = _normalize_indices(result.get(\"uk_company_indices\", []), n)\n",
    "\n",
    "    non_set = set(non_idx)\n",
    "    uk_idx = [i for i in uk_idx if i in non_set]\n",
    "\n",
    "    non_insts = _indices_to_institutions(row_list, non_idx)\n",
    "    acad_insts = _indices_to_institutions(row_list, acad_idx)\n",
    "    uk_insts = _indices_to_institutions(row_list, uk_idx)\n",
    "\n",
    "    org_indices = set(non_idx) | set(acad_idx)\n",
    "    uk_only = 1 if org_indices and org_indices.issubset(set(uk_idx)) else 0\n",
    "\n",
    "    return {\n",
    "        \"non_academic_indices\": non_idx,\n",
    "        \"academic_indices\": acad_idx,\n",
    "        \"uk_company_indices\": uk_idx,\n",
    "        \"non_academic_institutions\": non_insts,\n",
    "        \"academic_institutions\": acad_insts,\n",
    "        \"uk_company_institutions\": uk_insts,\n",
    "        \"non_academic_flag\": 1 if non_insts else 0,\n",
    "        \"academic_flag\": 1 if acad_insts else 0,\n",
    "        \"uk_company_flag\": 1 if uk_insts else 0,\n",
    "        \"uk_company_only_flag\": uk_only,\n",
    "        \"academic_uk_company_collab_flag\": 1 if acad_insts and uk_insts else 0,\n",
    "    }\n",
    "\n",
    "\n",
    "if () in unique_keys:\n",
    "    results_map = {(): normalize_result([], BASE_RESULT)}\n",
    "    unique_keys = [k for k in unique_keys if k]\n",
    "else:\n",
    "    results_map = {}\n",
    "\n",
    "cache_written_keys = set(results_map.keys())\n",
    "\n",
    "\n",
    "def load_cache():\n",
    "    if not LOAD_CACHE or not CACHE_PATH.exists():\n",
    "        return 0\n",
    "    loaded = 0\n",
    "    with CACHE_PATH.open() as handle:\n",
    "        for line in handle:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                record = json.loads(line)\n",
    "            except Exception:\n",
    "                continue\n",
    "            key = tuple(record.get(\"key\", []))\n",
    "            result = record.get(\"result\", {})\n",
    "            results_map[key] = normalize_result(list(key), result)\n",
    "            cache_written_keys.add(key)\n",
    "            loaded += 1\n",
    "    return loaded\n",
    "\n",
    "\n",
    "cached = load_cache()\n",
    "if cached:\n",
    "    unique_keys = [k for k in unique_keys if k not in cache_written_keys]\n",
    "    if VERBOSE:\n",
    "        print(f\"Loaded {cached} cached rows; remaining {len(unique_keys)} unique rows\")\n",
    "\n",
    "\n",
    "def append_cache_entries(batch_keys):\n",
    "    if not APPEND_CACHE or not batch_keys:\n",
    "        return\n",
    "    with CACHE_PATH.open(\"a\") as handle:\n",
    "        for key in batch_keys:\n",
    "            if key in cache_written_keys:\n",
    "                continue\n",
    "            result = results_map.get(key)\n",
    "            if result is None:\n",
    "                continue\n",
    "            record = {\"key\": list(key), \"result\": result}\n",
    "            handle.write(json.dumps(record, ensure_ascii=True) + \"\\n\")\n",
    "            cache_written_keys.add(key)\n",
    "\n",
    "\n",
    "def apply_results_to_df():\n",
    "    results = []\n",
    "    for key in row_keys:\n",
    "        result = results_map.get(key)\n",
    "        if result is None:\n",
    "            result = normalize_result(list(key), BASE_RESULT)\n",
    "        results.append(result)\n",
    "\n",
    "    df_work[\"non_academic_indices\"] = [r[\"non_academic_indices\"] for r in results]\n",
    "    df_work[\"academic_indices\"] = [r[\"academic_indices\"] for r in results]\n",
    "    df_work[\"uk_company_indices\"] = [r[\"uk_company_indices\"] for r in results]\n",
    "\n",
    "    df_work[\"non_academic_institutions\"] = [r[\"non_academic_institutions\"] for r in results]\n",
    "    df_work[\"academic_institutions\"] = [r[\"academic_institutions\"] for r in results]\n",
    "    df_work[\"uk_company_institutions\"] = [r[\"uk_company_institutions\"] for r in results]\n",
    "\n",
    "    df_work[\"non_academic_flag\"] = [r[\"non_academic_flag\"] for r in results]\n",
    "    df_work[\"academic_flag\"] = [r[\"academic_flag\"] for r in results]\n",
    "    df_work[\"uk_company_flag\"] = [r[\"uk_company_flag\"] for r in results]\n",
    "    df_work[\"uk_company_only_flag\"] = [r[\"uk_company_only_flag\"] for r in results]\n",
    "    df_work[\"academic_uk_company_collab_flag\"] = [\n",
    "        r[\"academic_uk_company_collab_flag\"] for r in results\n",
    "    ]\n",
    "    return df_work\n",
    "\n",
    "\n",
    "def save_snapshot(batch_keys=None, progress=False, excel=False):\n",
    "    if batch_keys:\n",
    "        append_cache_entries(batch_keys)\n",
    "    if progress or excel:\n",
    "        apply_results_to_df()\n",
    "        if progress and SAVE_PROGRESS_CSV:\n",
    "            df_work.to_csv(PROGRESS_CSV_PATH, index=False)\n",
    "        if excel:\n",
    "            df_work.to_excel(OUTPUT_PATH, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run classification and save incrementally\n",
    "\n",
    "Classify each batch via the LLM and save progress after each batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def classify_batch_with_fallback(batch_rows, depth=0):\n",
    "    if not batch_rows:\n",
    "        return []\n",
    "    if all(len(r) == 0 for r in batch_rows):\n",
    "        return [BASE_RESULT for _ in batch_rows]\n",
    "\n",
    "    try:\n",
    "        return await classify_non_academic_batch_async(batch_rows)\n",
    "    except Exception:\n",
    "        if len(batch_rows) > 1 and depth < MAX_FALLBACK_SPLITS:\n",
    "            mid = len(batch_rows) // 2\n",
    "            left = await classify_batch_with_fallback(batch_rows[:mid], depth + 1)\n",
    "            right = await classify_batch_with_fallback(batch_rows[mid:], depth + 1)\n",
    "            return left + right\n",
    "        if STRICT_RETRY:\n",
    "            try:\n",
    "                return await classify_non_academic_batch_async(batch_rows, strict=True)\n",
    "            except Exception:\n",
    "                failed_rows.extend(batch_rows)\n",
    "                return [BASE_RESULT for _ in batch_rows]\n",
    "        failed_rows.extend(batch_rows)\n",
    "        return [BASE_RESULT for _ in batch_rows]\n",
    "\n",
    "\n",
    "async def process_batches(batches):\n",
    "    semaphore = asyncio.Semaphore(MAX_CONCURRENCY)\n",
    "    save_lock = asyncio.Lock()\n",
    "    batch_counter = 0\n",
    "\n",
    "    pbar = tqdm(\n",
    "        total=len(unique_keys),\n",
    "        desc=\"Classifying\",\n",
    "        unit=\"row\",\n",
    "        mininterval=0.1,\n",
    "        miniters=1,\n",
    "    )\n",
    "\n",
    "    async def handle_batch(batch_keys):\n",
    "        nonlocal batch_counter\n",
    "        batch_rows = [list(k) for k in batch_keys]\n",
    "        async with semaphore:\n",
    "            batch_results = await classify_batch_with_fallback(batch_rows)\n",
    "        if len(batch_results) != len(batch_keys):\n",
    "            raise ValueError(\"Batch result length mismatch.\")\n",
    "        for key, result in zip(batch_keys, batch_results):\n",
    "            results_map[key] = normalize_result(list(key), result)\n",
    "        if REQUEST_SLEEP:\n",
    "            await asyncio.sleep(REQUEST_SLEEP)\n",
    "        async with save_lock:\n",
    "            batch_counter += 1\n",
    "            do_progress = (\n",
    "                SAVE_PROGRESS_CSV\n",
    "                and SAVE_PROGRESS_EVERY_N\n",
    "                and batch_counter % SAVE_PROGRESS_EVERY_N == 0\n",
    "            )\n",
    "            do_excel = (\n",
    "                SAVE_EXCEL_EVERY_N\n",
    "                and batch_counter % SAVE_EXCEL_EVERY_N == 0\n",
    "            )\n",
    "            save_snapshot(batch_keys, progress=do_progress, excel=bool(do_excel))\n",
    "        pbar.update(len(batch_keys))\n",
    "\n",
    "    tasks = [asyncio.create_task(handle_batch(batch)) for batch in batches]\n",
    "    if tasks:\n",
    "        await asyncio.gather(*tasks)\n",
    "    pbar.close()\n",
    "    return results_map\n",
    "\n",
    "\n",
    "batches = list(make_batches(unique_keys, BATCH_SIZE))\n",
    "start = time.time()\n",
    "\n",
    "if batches:\n",
    "    results_map = await process_batches(batches)\n",
    "\n",
    "elapsed = time.time() - start\n",
    "if row_keys and VERBOSE:\n",
    "    req_count = len(batches)\n",
    "    row_rate = len(unique_keys) / elapsed if elapsed > 0 else 0\n",
    "    req_rate = req_count / elapsed if elapsed > 0 else 0\n",
    "    print(\n",
    "        f\"Processed {len(unique_keys)} API rows in {elapsed:.1f}s \"\n",
    "        f\"across {req_count} requests (~{row_rate:.2f} rows/s, ~{req_rate:.2f} req/s)\"\n",
    "    )\n",
    "\n",
    "if failed_rows and VERBOSE:\n",
    "    print(f\"Warning: {len(failed_rows)} rows failed to parse and were set empty.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final snapshot\n",
    "\n",
    "Write the final results after all batches complete.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/porco/Dropbox/20_years_of_ukb/output/non_academic_flagged.xlsx')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_snapshot(progress=True, excel=SAVE_EXCEL_AT_END)\n",
    "OUTPUT_PATH\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seeds311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
